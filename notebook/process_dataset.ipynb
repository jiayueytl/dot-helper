{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd304d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9253c438",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_multi = pd.read_json(r'C:\\Users\\jiayue.tan\\OneDrive - YTL\\Workstation\\20251002LanguageFollowing_V2\\text_processing_ext_ref_multi_turn\\all_v2.jsonl',lines=True)\n",
    "tp_multi['source'] = 'text_processing_ext_ref_multi_turn'\n",
    "\n",
    "tp_single = pd.read_json(r'C:\\Users\\jiayue.tan\\OneDrive - YTL\\Workstation\\20251002LanguageFollowing_V2\\text_processing_ext_ref_single_turn\\all_v2.jsonl',lines=True)\n",
    "tp_single['source'] = 'text_processing_ext_ref_single_turn'\n",
    "\n",
    "tp = pd.concat([tp_multi,tp_single])\n",
    "tp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd179144",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_total_word_count(question: str, answer: str, reason: str = \"\") -> int:\n",
    "    combined_text = f\"{question}{answer}{reason}\"\n",
    "    \n",
    "    english_number_words = re.findall(r\"[a-zA-Z0-9']+\", combined_text)\n",
    "    english_number_word_count = len(english_number_words)\n",
    "    \n",
    "    chinese_chars = re.findall(r'[\\u4e00-\\u9fff]', combined_text)\n",
    "    chinese_char_count = len(chinese_chars)\n",
    "    \n",
    "    total_count = english_number_word_count + chinese_char_count\n",
    "    return total_count\n",
    "\n",
    "def get_turn_languages_standard(data_list):\n",
    "    languages = []\n",
    "    for item in data_list[1:]:\n",
    "        metadata = item.get('metadata')\n",
    "        if metadata:\n",
    "            languages.append(metadata.get('language', 'N/A'))\n",
    "    return languages\n",
    "\n",
    "def flag_if_list_contains_chinese(language_list: list) -> int:\n",
    "    \n",
    "    if not isinstance(language_list, list):\n",
    "        return 0\n",
    "    for item in language_list:\n",
    "        if isinstance(item, str) and \"chinese\" in item.lower():\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e390404",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp['original_id'] = \"langfol1014_\" + (tp.index + 1).astype(str).str.zfill(5)\n",
    "tp['sft_round'] = \"LanguageFollowing\"\n",
    "tp['question'] = tp['prompt']\n",
    "tp['answer'] = tp['response']\n",
    "\n",
    "tp['language'] = tp['reference'].apply(lambda x:x.get('language'))\n",
    "# The original lambda, modified to start from the second item\n",
    "\n",
    "# 1. Define a function that does the work for a single row's reference dictionary\n",
    "def extract_languages_from_reference(ref_dict):\n",
    "    \"\"\"\n",
    "    Safely extracts 'turn_language' from a single reference dictionary.\n",
    "    Returns a list of languages or ['N/A'] if no data is found.\n",
    "    \"\"\"\n",
    "    # Get the list of metadata, defaulting to an empty list if the key is missing\n",
    "    metadata_list = ref_dict.get('all_turn_metadata', [])\n",
    "    \n",
    "    # If the list is empty, return a list with a single 'N/A' placeholder\n",
    "    if not metadata_list:\n",
    "        return ['N/A']\n",
    "        \n",
    "    # If the list is not empty, use the lambda to extract languages\n",
    "    return [item.get('metadata', {}).get('turn_language', 'N/A') for item in metadata_list]\n",
    "\n",
    "tp['turn_languages'] = tp['reference'].apply(extract_languages_from_reference)\n",
    "tp['chinese_flag'] = tp['turn_languages'].apply(flag_if_list_contains_chinese)\n",
    "\n",
    "tp['style'] = tp['reference'].apply(lambda x: x.get('style'))\n",
    "tp['rendered_history'] = tp['history'].apply(render_messages_safe)\n",
    "\n",
    "tp['reason'] = tp['rendered_history']\n",
    "tp['task'] = \"LANGUAGE: \" + tp['language'].astype(str) + \" | STYLE: \" + tp['style'].astype(str) + \" | SOURCE: \" + tp['source']\n",
    "tp['domain'] = tp['language']\n",
    "tp['total_word_count'] = tp.apply(lambda row: get_total_word_count(row['question'], row['answer'], row['reason']), axis=1)\n",
    "\n",
    "tp2 = tp[['original_id','sft_round','question','answer','reason','turn_languages','chinese_flag','task','domain','style','total_word_count']]\n",
    "\n",
    "def classify_wordcount_class(total_words_count):\n",
    "    if total_words_count <= 600:\n",
    "        return 'short(1-600)'\n",
    "    elif total_words_count>=601 and total_words_count <= 1100:\n",
    "        return 'medium(601-1100)'\n",
    "    elif total_words_count>=1101 and total_words_count <= 1900:\n",
    "        return 'long(1101-1900)'\n",
    "    elif total_words_count>=1901:\n",
    "        return 'very_long(>1901)'\n",
    "    else: \n",
    "        return 'you_tell_me'\n",
    "    \n",
    "tp2['wordcount_class'] = tp2['total_word_count'].apply(classify_wordcount_class)\n",
    "tp2.groupby(['wordcount_class']).size().reset_index(name='counts').sort_values(by='counts', ascending=False)\n",
    "\n",
    "tp2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb2cef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def create_balanced_samples_flexible(\n",
    "    df: pd.DataFrame,\n",
    "    stratify_cols: list,\n",
    "    samples_per_annotator: int,\n",
    "    num_annotators: int,\n",
    "    group_id_col: str = 'package_id',\n",
    "    random_state: int = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a balanced dataset for annotators, adjusting if not enough data is available.\n",
    "    If requested samples exceed data size, it uses ceiling logic and distributes until data runs out.\n",
    "    \"\"\"\n",
    "    if not all(col in df.columns for col in stratify_cols):\n",
    "        raise ValueError(\"One or more columns in stratify_cols are not in the DataFrame.\")\n",
    "    if samples_per_annotator <= 0 or num_annotators <= 0:\n",
    "        raise ValueError(\"Samples per annotator and number of annotators must be positive.\")\n",
    "\n",
    "    total_samples_needed = samples_per_annotator * num_annotators\n",
    "\n",
    "    if total_samples_needed > len(df):\n",
    "        # ceil version — try to spread evenly until data runs out\n",
    "        max_possible_per_annotator = math.ceil(len(df) / num_annotators)\n",
    "        print(f\"⚠️ WARNING: Not enough data for full request.\")\n",
    "        print(f\"Requested {total_samples_needed} samples, but only {len(df)} available.\")\n",
    "        print(f\"Adjusting 'samples_per_annotator' to {max_possible_per_annotator} (ceil logic).\")\n",
    "        samples_per_annotator = max_possible_per_annotator\n",
    "\n",
    "    # Compute total possible\n",
    "    total_samples_needed = min(samples_per_annotator * num_annotators, len(df))\n",
    "\n",
    "    grouped = df.groupby(stratify_cols)\n",
    "    master_sample_list = []\n",
    "\n",
    "    for _, group_df in grouped:\n",
    "        stratum_size = len(group_df)\n",
    "        samples_from_stratum = round((stratum_size / len(df)) * total_samples_needed)\n",
    "        samples_from_stratum = min(samples_from_stratum, stratum_size)\n",
    "\n",
    "        if samples_from_stratum > 0:\n",
    "            sample = group_df.sample(n=samples_from_stratum, random_state=random_state)\n",
    "            master_sample_list.append(sample)\n",
    "\n",
    "    master_pool_df = pd.concat(master_sample_list, ignore_index=True)\n",
    "\n",
    "    # If under or over\n",
    "    current_pool_size = len(master_pool_df)\n",
    "    if current_pool_size < total_samples_needed:\n",
    "        remaining_df = df[~df.index.isin(master_pool_df.index)]\n",
    "        if len(remaining_df) > 0:\n",
    "            additional_n = min(total_samples_needed - current_pool_size, len(remaining_df))\n",
    "            master_pool_df = pd.concat(\n",
    "                [master_pool_df, remaining_df.sample(n=additional_n, random_state=random_state)],\n",
    "                ignore_index=True\n",
    "            )\n",
    "    elif current_pool_size > total_samples_needed:\n",
    "        master_pool_df = master_pool_df.sample(n=total_samples_needed, random_state=random_state)\n",
    "\n",
    "    # Shuffle and assign group IDs\n",
    "    master_pool_df = master_pool_df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "    # Assign groups evenly until data runs out\n",
    "    master_pool_df[group_id_col] = (np.arange(len(master_pool_df)) // samples_per_annotator) + 1\n",
    "    master_pool_df[group_id_col] = master_pool_df[group_id_col].clip(upper=num_annotators)\n",
    "\n",
    "    return master_pool_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279b52bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "stratification_columns = ['domain', 'wordcount_class']\n",
    "samples_for_each_packages = 37.5\n",
    "number_of_packages = 20\n",
    "\n",
    "# --- 3. Run the Function ---\n",
    "annotator_dfs = create_balanced_samples_flexible(\n",
    "    tp2,\n",
    "    stratify_cols=stratification_columns,\n",
    "    samples_per_annotator=samples_for_each_packages,\n",
    "    num_annotators=number_of_packages,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"DataFrame with Annotator Group Assignments:\")\n",
    "# print(df_with_groups)\n",
    "print(\"-\" * 30)\n",
    "\n",
    "annotator_dfs.groupby(['wordcount_class','package_id']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d3b789",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator_dfs.groupby(['package_id']).size().reset_index(name='counts').sort_values(by=['package_id'], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69a4293",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator_dfs.groupby(['package_id']).size().reset_index(name='counts').sort_values(by='counts', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
